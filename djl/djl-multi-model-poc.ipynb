{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Standard instruction for using LMI container on SageMaker\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade  --quiet\n",
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e115eb-e1a4-4d1e-bb22-d4aa219a0891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# function to download model and upload to S3\n",
    "def download_model(bucket, model_id, commit_hash=None):\n",
    "    from huggingface_hub import snapshot_download\n",
    "    from pathlib import Path\n",
    "\n",
    "    local_model_folder_name = f\"LLM_{model_id.replace('/', '_')}_model\"\n",
    "    s3_model_prefix = f\"LLM/{local_model_folder_name}\"  # folder where model checkpoint will go\n",
    "\n",
    "    local_model_path = Path(local_model_folder_name)\n",
    "\n",
    "    local_model_path.mkdir(exist_ok=True)\n",
    "    snapshot_download(repo_id=model_id, revision=commit_hash, cache_dir=local_model_path, allow_patterns=[\"*.md\", \"*.json\", \"*.bin\", \"*.txt\"])\n",
    "\n",
    "    model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "\n",
    "    print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "\n",
    "    os.system(f'aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666c9df-71e9-4eb8-8fea-3f7b0fb83559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_id in [\"facebook/opt-350m\", \"bigscience/bloomz-560m\"]:\n",
    "    download_model(bucket, model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19d6798b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "predictor = None\n",
    "models = {}\n",
    "\n",
    "def my_predict(data):\n",
    "    model_opt = models['LLM_facebook_opt-350m_model']\n",
    "    model_bloom = models['LLM_bigscience_bloomz-560m_model']\n",
    "\n",
    "    result_opt = model_opt(data, do_sample=True)\n",
    "    result_bloom = model_bloom(data, do_sample=True)\n",
    "\n",
    "    return {\n",
    "        'opt': result_opt,\n",
    "        'bloom': result_bloom\n",
    "    }\n",
    "\n",
    "def init_models(properties):\n",
    "    \"\"\"load all models\"\"\"\n",
    "    device_id = properties.get('device_id')\n",
    "    model_base_id = properties.get('model_id')\n",
    "\n",
    "    for model_id in [\"LLM_facebook_opt-350m_model\", \"LLM_bigscience_bloomz-560m_model\"]:\n",
    "        local_model_dir = f'{model_base_id}/{model_id}'\n",
    "\n",
    "        dtype = torch.float16\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_model_dir, torch_dtype=dtype)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "        # specify a device id.\n",
    "        generator = pipeline(task='text-generation', model=local_model_dir, tokenizer=tokenizer, device=f'cuda:{device_id}')\n",
    "\n",
    "        models[model_id] = generator\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        properties = inputs.get_properties()\n",
    "        logging.info('init models with properties:', properties)\n",
    "        # init models\n",
    "        init_models(properties)\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()['prompt']\n",
    "    try:\n",
    "        result = my_predict(data)\n",
    "    except Exception as err:\n",
    "        logging.info(err)\n",
    "        raise err\n",
    "\n",
    "    result = {'ipt_properties': inputs.get_properties(), 'r': result}\n",
    "\n",
    "    return Output().add(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80da0bfa-bb65-4896-825c-430cbc73dd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this config will load a model copy on each GPU card, i.e run model.py with different device_id.\n",
    "with open('serving.properties', 'w') as f:\n",
    "    f.write(f\"\"\"engine=Python\n",
    "option.model_id=s3://{bucket}/LLM/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a174b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "671ea485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.py\n",
      "requirements.txt\n",
      "serving.properties\n"
     ]
    }
   ],
   "source": [
    "! tar -czvf model.tar.gz model.py requirements.txt serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238c006-12e7-4a8e-8578-ffd0fb73deeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload model data to S3\n",
    "model_s3_path = f\"s3://{bucket}/LLM-code/multi-models/0406/model.tar.gz\"\n",
    "!aws s3 cp model.tar.gz $model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38b1e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(image_uri=image_uri, model_data=model_s3_path, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "## Step 4 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e0e61cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"  # g5.12xlarge instance has 4 GPUs\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-multi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference\n",
    "\n",
    "Note the 'device_id' of 'ipt_properties'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bcef095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ipt_properties': {'Accept': 'application/json', 'Content-Length': '38', 'Content-Type': 'application/json', 'handler': 'handle', 'Host': '169.254.180.2:8080', 'User-Agent': 'AHC/2.0', 'device_id': '0', 'model_dir': '/opt/ml/model'}, 'r': {'opt': [{'generated_text': \"Large model inference is supported to help understand the model's complexity. Large models are widely used in scientific\"}], 'bloom': [{'generated_text': 'Large model inference is a method that yields large estimators for the parameters of a given model; with'}]}}\n",
      "{'ipt_properties': {'Accept': 'application/json', 'Content-Length': '38', 'Content-Type': 'application/json', 'handler': 'handle', 'Host': '169.254.180.2:8080', 'User-Agent': 'AHC/2.0', 'device_id': '1', 'model_dir': '/opt/ml/model'}, 'r': {'opt': [{'generated_text': 'Large model inference is a complex and confusing problem, and our work shows that large models may be the'}], 'bloom': [{'generated_text': 'Large model inference is often used by scientists, who are interested in testing hypotheses about biology.'}]}}\n",
      "{'ipt_properties': {'Accept': 'application/json', 'Content-Length': '38', 'Content-Type': 'application/json', 'handler': 'handle', 'Host': '169.254.180.2:8080', 'User-Agent': 'AHC/2.0', 'device_id': '2', 'model_dir': '/opt/ml/model'}, 'r': {'opt': [{'generated_text': 'Large model inference is based on the assumption that the model is completely random. Large sets of predictions are'}], 'bloom': [{'generated_text': 'Large model inference is performed along with standard inference; the model is the solution of a system of equations'}]}}\n",
      "{'ipt_properties': {'Accept': 'application/json', 'Content-Length': '38', 'Content-Type': 'application/json', 'handler': 'handle', 'Host': '169.254.180.2:8080', 'User-Agent': 'AHC/2.0', 'device_id': '3', 'model_dir': '/opt/ml/model'}, 'r': {'opt': [{'generated_text': 'Large model inference is a technique that has been developed in computer science to learn from data. It allows'}], 'bloom': [{'generated_text': 'Large model inference is needed to understand the interplay between population genetic and environmental factors. A major goal'}]}}\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict( {\"prompt\": \"Large model inference is\"}))\n",
    "print(predictor.predict( {\"prompt\": \"Large model inference is\"}))\n",
    "print(predictor.predict( {\"prompt\": \"Large model inference is\"}))\n",
    "print(predictor.predict( {\"prompt\": \"Large model inference is\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d674b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ab663-dbcf-4f42-9121-f5f312f7b8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
